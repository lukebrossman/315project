
The proliferation of the internet means that documents can be copied partially or in whole in nearly unlimited ways and an unlimited number of times. Thus it is important to have a reliable algorithm for detecting matches between 2 or more documents. One method for comparing documents is to create document fingerprints for every document and search other documents for occurrences of those fingerprints. This can be done by parsing the document into contiguous sets of K characters called K-grams. The K-grams are then hashed, and a subset of them is selected using a method such as 0 mod p, so that 1/p K-gram hash values are retained as that documents fingerprints. This method is extremely simple, but has a relatively high likelihood of producing false negatives, because the gap between selected hash values is unbounded and could contain the copied characters. One method to mitigate this risk is to segregate the document into windows of w consecutive K-grams and select a minimum of one hash value from each window to use as a fingerprint.

There are other methods of document matching that do not rely on creating fingerprints of K-grams for the documents. However all of these algorithms contain essentially the same first step. First the documents are trimmed to remove white-space and noise such as insignificant similarities between the documents. The next step is where different methods of document matching come in. One method is to compare all pairs of K-grams between documents, or some subset of all pairs, but this method is weak against reordering of copied material and position in the document. Another method is to compare sentences or snippets of the document that begin with “anchor” words or phrases or characters, however this method requires tailoring for nearly every document type.

The winnowing algorithm in this paper is considered the baseline for K-gram fingerprint selecting. At its core, the winnowing algorithm is unique because it uses a threshold value, t, to denote the minimum length of substrings in a match that are considered non trivial. Matches between substrings shorter than the threshold t are likely noise, and as such are not considered true matches. When using the winnowing algorithm it is up to the discretion of the user to pick values for k and t. The larger the chosen t, the lower the false positive rate, but the higher the false negative rate.

The researchers of this paper aim to improve performance over the winnowing algorithm through the use of what they call, “local algorithms”. The sense that I have of this concept is that the selection algorithm used to specify a fingerprint for a given document changes based on the exact window of hash values from said document. The basis of this idea is that there are long sections of generic, “low-entropy” strings on the web that can create a high degree of false positive matching. Local algorithmic approach to fingerprinting a document allows large portions of documents with a low variance of hash values to be discarded or weighted very lowly so that little or no fingerprints are selected from these stretches.


