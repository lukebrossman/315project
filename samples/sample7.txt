
For every point x there exists a distance d+  and d- such that d+ is the distance to C+ for the positive class and d- is the distance to C- for the negative class. If d+ and d- are the same, then that implies that the point x is directly on the decision boundary between the classes. The minimum distances d+ and d- such that d+ = d- is the middle point )y) on the line directly between C+ and C-. every point x that is equal distance from C+ and C- will form an isosceles triangle with the points C+ and C-. If you divide this triangle in 2 with a line that passes through point y you will get two right triangles where the angle between the dividing line and the line between centers is 90 degrees. This is true for any point x, which implies that for every feature or dimension, in the dataset, a point x that is equal distance from C- and C+ will fall somewhere on a line perpendicular to the midline between centers. The combination of all of these lines perpendicular to the midline is a plane in N dimensions, where the slope of the line in dimension Ni is equal to the weight of that feature. Hence the decision boundary is a linear hyperplane of the form W * X where W is the set of weights i-n and X is the set of features i-n.
To begin, despite the existence of thousands of machine learning algorithms, they can all be described by a combination of 3 components. A Representation component, an Evaluation component, and an optimization component. The representation component exists as a way to communicate a physical real world sample, event, or scenario to a digital, binary machine. In essence this can be described as distilling the sample data into quantifiable and distinct events that a computer can distinguish between. The evaluation component is the method by which the machine learning algorithm gives the user an answer to their question. That is to say, when provided with a representation of an event, sample, or scenario that was previously unseen, the algorithm will evaluate it and make a prediction about what the correct answer is to the given question. Finally the optimization component, which is where the learning happens, is the method through which a machine learning algorithm picks one variant of the evaluation component over another.
The important metric of a machine learning algorithm is how it performs on unseen data
not how it performs on data that has already been collected and classified. This means that more generalized classifiers tend to outperform more complicated and specific ones in real world scenarios. For the most part this discrepancy is caused by the fact that almost all real world scenarios consist primarily of unseen and unique samples, which is to say the likelihood of seeing data that matches training data is extremely low.	
      A famous theorem in the field of machine learning is the idea of “no free lunch”. This theorem essentially shows that there is no one algorithm that can always perform well in any given problem space, “on it's own”. More specifically, in order for a machine learning algorithm to perform well, it requires some starter knowledge and assumptions from the user. These assumptions range from which algorithm to use in a given problem space all the way down to the customizable parameters in each specific classifier.
      One of the largest problems in machine learning is called the curse of dimensionality. The curse is that as the number of relevant features that affect a class label grows, it becomes more and more difficult to find “similar” data-points. In other words, as a dataset grows in dimensions, the similarity between samples in the data-set shrinks. This problem is quite pervasive because in the physical world, the dimensions of a specific scenario can be essentially as large as we want them to be. This curse is partially countered by the fact that data tends not to be uniformly distributed, despite the near limitless size and dimensions of any given problem. 
      The key to many of the problems discussed above is a process known as “feature engineering”. This is the process of selectively measuring, and trimming features that are either not relevant to the question asked, or introduced to high a degree of bias into the algorithm. It is possible to perform feature engineering with automated processes, but it is an extremely inefficient and costly process. This is because without a trained observer, often times brute force is the only solution an algorithm can employ. As of yet, the best way to perform feature engineering still comes from human beings with trained senses and intuitions about problems and data-sets, automating only the higher dimensional problems that evade a humans ability to foresee.
      The last takeaway from this paper is that in general, more is always better when it comes to machine learning. More data beats less data, and more models beats less models. More data is better than less because of the tendency for small data-sets to capture oddities and bias that is not reflected in the overall problem space. The more data you have, the more generalized the resultant classifier is. The justification for more models comes from humans inherit inability to think in high dimensions. Learning multiple models can help to overcome the urge to use models that only “seem” to fit because of our narrow range of perception.
